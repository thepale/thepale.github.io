<h2>The AI Cult</h2>
As anyone who has used SIRI for long enough will know, AI does not quite work yet. That doesn't necessarily mean the people working on AI are doing a bad job - it's a hard problem. But I worry that sometimes the probabilistic nature of today's AI is used as an excuse for the laziness of its creators. Presumably Apple employees run into the same issues with SIRI that we do, so why aren't they constantly fixing these problems? In ordinary software development, bugs that real users encounter are considered both extremely bad and fixable, so developers generally aim to have no serious bugs in anything they release. In AI it may be infeasible to create products that work 100% of the time, but the danger with that thought is that it makes it too easy to excuse problems that can be fixed.
<br><br>
Organized religion has its problems, but it creates a social structure far better than that of a cult. A common technique used by the authority in a cult is to hand down punishments and rewards randomly, sometimes alternating between punishment and reward for the same actions. Eventually the members will lose confidence in their ability to evaluate their own actions, and will be unlikely to challenge the authority's judgments, no matter how wrong they would seem to an outsider. A more healthy religion would have a set of written rules that anyone could hold the authority accountable to. Conversely, members could evaluate their own actions, since written rules change rarely, if at all. 
<br><br>
At its worst, AI is practiced as an algorithms cult. There is a fixed set of algorithms, and you attempt to solve your problem by making calls to them. Even after you've input as much good data as you can, your solution may not work, but that's not your fault - the algorithms weren't good enough. There is very little you can do to improve the algorithms - nobody understands why the existing algorithms work, so any attempt at improvement would be a guessing game. You might be able to squeeze out an extra couple percent if you tune your parameters just right.
<br><br>
This is similar to the attitude of many biotech companies, where chemical compounds are worshipped in place of algorithms. You can't improve the properties of a chemical compound, you can only run tests to check whether they exist. You can identify promising new chemical compounds, but there aren't any intelligent ways of doing this - the usual method is to create many millions of chemicals at random and check which ones do what you want. Any field that roughly fits this template may be in trouble, because the biotech industry over the last couple decades has been a disappointment, relative not just to the high expectations placed on it, but also to reasonable expectations of progress. There are no large VC firms that focus on biotech - they've all gone out of business or moved to other industries. It costs a billion dollars to develop a new drug and get it approved by the FDA. 
<br><br>
Does this mean that the AI field is doomed, or at least that the most recent AI boom is some kind of bubble? Not necessarily. AI does not have to look like Biology - it is closely related to software development and Math, which are much more deterministic fields. Also, the events that spawned the current AI boom, the creation of ImageNet and AlexNet, did not fit the cult-pattern. One organization decided that image recognition was an important problem, and gave everyone else lots of data, so they could work on it. Then a research lab developed a breakthrough that, combined with a couple years of incremental improvements, solved the problem. Much of the cult-behavior today stems from the fact that we do not really understand the breakthrough, and it is sometimes assumed, implicitly, that there is nothing to understand - it was a stroke of luck that we discovered these new techniques, and questioning them will not get us anywhere. It may be quite difficult to understand why, for example, the AlexNet architecture needed 5 convolutional layers rather than 4. But further progress is unlikely unless these things are better understood. Unquestioning ignorance eventually goes wrong, for the same reason cults are bad for their members.
